{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36429c62",
   "metadata": {},
   "source": [
    "# Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d5ed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./py312env/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in ./py312env/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: openpyxl in ./py312env/lib/python3.12/site-packages (3.1.5)\n",
      "Requirement already satisfied: spacy in ./py312env/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: matplotlib in ./py312env/lib/python3.12/site-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in ./py312env/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: networkx in ./py312env/lib/python3.12/site-packages (3.4.2)\n",
      "Requirement already satisfied: plotly in ./py312env/lib/python3.12/site-packages (5.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./py312env/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./py312env/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./py312env/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: et-xmlfile in ./py312env/lib/python3.12/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./py312env/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./py312env/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./py312env/lib/python3.12/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./py312env/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./py312env/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in ./py312env/lib/python3.12/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./py312env/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./py312env/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./py312env/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./py312env/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./py312env/lib/python3.12/site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./py312env/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./py312env/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./py312env/lib/python3.12/site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in ./py312env/lib/python3.12/site-packages (from spacy) (3.1.5)\n",
      "Requirement already satisfied: setuptools in ./py312env/lib/python3.12/site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./py312env/lib/python3.12/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./py312env/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./py312env/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./py312env/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./py312env/lib/python3.12/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./py312env/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in ./py312env/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./py312env/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in ./py312env/lib/python3.12/site-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: language-data>=1.2 in ./py312env/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./py312env/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./py312env/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./py312env/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in ./py312env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./py312env/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./py312env/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./py312env/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./py312env/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in ./py312env/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./py312env/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in ./py312env/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./py312env/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./py312env/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./py312env/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./py312env/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./py312env/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in ./py312env/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./py312env/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./py312env/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in ./py312env/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./py312env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in ./py312env/lib/python3.12/site-packages (4.48.1)\n",
      "Requirement already satisfied: torch in ./py312env/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: datasets in ./py312env/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in ./py312env/lib/python3.12/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./py312env/lib/python3.12/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./py312env/lib/python3.12/site-packages (from transformers) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./py312env/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./py312env/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./py312env/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./py312env/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./py312env/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./py312env/lib/python3.12/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./py312env/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./py312env/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./py312env/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./py312env/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./py312env/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in ./py312env/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./py312env/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./py312env/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./py312env/lib/python3.12/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./py312env/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./py312env/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in ./py312env/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./py312env/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./py312env/lib/python3.12/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./py312env/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./py312env/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./py312env/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./py312env/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./py312env/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./py312env/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./py312env/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./py312env/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./py312env/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./py312env/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./py312env/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./py312env/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./py312env/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./py312env/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./py312env/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./py312env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in ./py312env/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./py312env/lib/python3.12/site-packages (from scikit-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./py312env/lib/python3.12/site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./py312env/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./py312env/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting en-core-web-trf==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.8.0/en_core_web_trf-3.8.0-py3-none-any.whl (457.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy-curated-transformers<1.0.0,>=0.2.2 in ./py312env/lib/python3.12/site-packages (from en-core-web-trf==3.8.0) (0.3.0)\n",
      "Requirement already satisfied: curated-transformers<0.2.0,>=0.1.0 in ./py312env/lib/python3.12/site-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.1.1)\n",
      "Requirement already satisfied: curated-tokenizers<0.1.0,>=0.0.9 in ./py312env/lib/python3.12/site-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.0.9)\n",
      "Requirement already satisfied: torch>=1.12.0 in ./py312env/lib/python3.12/site-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.5.1)\n",
      "Requirement already satisfied: regex>=2022 in ./py312env/lib/python3.12/site-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2024.11.6)\n",
      "Requirement already satisfied: filelock in ./py312env/lib/python3.12/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./py312env/lib/python3.12/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./py312env/lib/python3.12/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./py312env/lib/python3.12/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./py312env/lib/python3.12/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in ./py312env/lib/python3.12/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./py312env/lib/python3.12/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./py312env/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./py312env/lib/python3.12/site-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_trf')\n",
      "Requirement already satisfied: huggingface_hub in ./py312env/lib/python3.12/site-packages (0.27.1)\n",
      "Requirement already satisfied: filelock in ./py312env/lib/python3.12/site-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./py312env/lib/python3.12/site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./py312env/lib/python3.12/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./py312env/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./py312env/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./py312env/lib/python3.12/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./py312env/lib/python3.12/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./py312env/lib/python3.12/site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./py312env/lib/python3.12/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./py312env/lib/python3.12/site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./py312env/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.12.14)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pyvis in ./py312env/lib/python3.12/site-packages (0.3.2)\n",
      "Requirement already satisfied: ipython>=5.3.0 in ./py312env/lib/python3.12/site-packages (from pyvis) (8.31.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in ./py312env/lib/python3.12/site-packages (from pyvis) (3.1.5)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in ./py312env/lib/python3.12/site-packages (from pyvis) (4.0.1)\n",
      "Requirement already satisfied: networkx>=1.11 in ./py312env/lib/python3.12/site-packages (from pyvis) (3.4.2)\n",
      "Requirement already satisfied: decorator in ./py312env/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./py312env/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./py312env/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./py312env/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./py312env/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./py312env/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (2.19.1)\n",
      "Requirement already satisfied: stack_data in ./py312env/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in ./py312env/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis) (5.14.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./py312env/lib/python3.12/site-packages (from jinja2>=2.9.6->pyvis) (3.0.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./py312env/lib/python3.12/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./py312env/lib/python3.12/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./py312env/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./py312env/lib/python3.12/site-packages (from stack_data->ipython>=5.3.0->pyvis) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./py312env/lib/python3.12/site-packages (from stack_data->ipython>=5.3.0->pyvis) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./py312env/lib/python3.12/site-packages (from stack_data->ipython>=5.3.0->pyvis) (0.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: dash in ./py312env/lib/python3.12/site-packages (2.18.2)\n",
      "Requirement already satisfied: Flask<3.1,>=1.0.4 in ./py312env/lib/python3.12/site-packages (from dash) (3.0.3)\n",
      "Requirement already satisfied: Werkzeug<3.1 in ./py312env/lib/python3.12/site-packages (from dash) (3.0.6)\n",
      "Requirement already satisfied: plotly>=5.0.0 in ./py312env/lib/python3.12/site-packages (from dash) (5.24.1)\n",
      "Requirement already satisfied: dash-html-components==2.0.0 in ./py312env/lib/python3.12/site-packages (from dash) (2.0.0)\n",
      "Requirement already satisfied: dash-core-components==2.0.0 in ./py312env/lib/python3.12/site-packages (from dash) (2.0.0)\n",
      "Requirement already satisfied: dash-table==5.0.0 in ./py312env/lib/python3.12/site-packages (from dash) (5.0.0)\n",
      "Requirement already satisfied: importlib-metadata in ./py312env/lib/python3.12/site-packages (from dash) (8.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in ./py312env/lib/python3.12/site-packages (from dash) (4.12.2)\n",
      "Requirement already satisfied: requests in ./py312env/lib/python3.12/site-packages (from dash) (2.32.3)\n",
      "Requirement already satisfied: retrying in ./py312env/lib/python3.12/site-packages (from dash) (1.3.4)\n",
      "Requirement already satisfied: nest-asyncio in ./py312env/lib/python3.12/site-packages (from dash) (1.6.0)\n",
      "Requirement already satisfied: setuptools in ./py312env/lib/python3.12/site-packages (from dash) (75.8.0)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in ./py312env/lib/python3.12/site-packages (from Flask<3.1,>=1.0.4->dash) (3.1.5)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in ./py312env/lib/python3.12/site-packages (from Flask<3.1,>=1.0.4->dash) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in ./py312env/lib/python3.12/site-packages (from Flask<3.1,>=1.0.4->dash) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.6.2 in ./py312env/lib/python3.12/site-packages (from Flask<3.1,>=1.0.4->dash) (1.9.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in ./py312env/lib/python3.12/site-packages (from plotly>=5.0.0->dash) (9.0.0)\n",
      "Requirement already satisfied: packaging in ./py312env/lib/python3.12/site-packages (from plotly>=5.0.0->dash) (24.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./py312env/lib/python3.12/site-packages (from Werkzeug<3.1->dash) (3.0.2)\n",
      "Requirement already satisfied: zipp>=3.20 in ./py312env/lib/python3.12/site-packages (from importlib-metadata->dash) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./py312env/lib/python3.12/site-packages (from requests->dash) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./py312env/lib/python3.12/site-packages (from requests->dash) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./py312env/lib/python3.12/site-packages (from requests->dash) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./py312env/lib/python3.12/site-packages (from requests->dash) (2024.12.14)\n",
      "Requirement already satisfied: six>=1.7.0 in ./py312env/lib/python3.12/site-packages (from retrying->dash) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: rapidfuzz in ./py312env/lib/python3.12/site-packages (3.11.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy openpyxl spacy matplotlib seaborn networkx plotly\n",
    "!pip install transformers torch datasets\n",
    "!pip install scikit-learn\n",
    "\n",
    "!python -m spacy download en_core_web_trf\n",
    "!pip install huggingface_hub\n",
    "!pip install pyvis #network graph\n",
    "!pip install dash #interactive dashboard\n",
    "!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78a9ce31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zuwei/py312env/lib/python3.12/site-packages/thinc/shims/pytorch.py:261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from rapidfuzz import fuzz\n",
    "from collections import Counter\n",
    "\n",
    "# Load advanced SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b4463",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a35f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel(\"Documents/news_excerpts_parsed.xlsx\")[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a32037a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>relationships</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://edition.cnn.com/2023/09/29/business/st...</td>\n",
       "      <td>Starbucks violated federal labor law when it i...</td>\n",
       "      <td>[starbucks, violate, federal, labor, law, incr...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.channelnewsasia.com/singapore/su-w...</td>\n",
       "      <td>The first suspect to plead guilty in Singapore...</td>\n",
       "      <td>[suspect, plead, guilty, singapores, large, mo...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://edition.cnn.com/2023/05/22/tech/meta-f...</td>\n",
       "      <td>Meta has been fined a record-breaking €1.2 bil...</td>\n",
       "      <td>[meta, fine, recordbreaking, billion, billion,...</td>\n",
       "      <td>[{'source': 'chapter', 'relation': 'set', 'tar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.channelnewsasia.com/singapore/bill...</td>\n",
       "      <td>SINGAPORE: A 45-year-old man linked to Singapo...</td>\n",
       "      <td>[singapore, yearold, man, link, singapores, la...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://edition.cnn.com/2024/03/05/politics/li...</td>\n",
       "      <td>The Department of Education imposed a record $...</td>\n",
       "      <td>[department, education, impose, record, millio...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Link  \\\n",
       "0  https://edition.cnn.com/2023/09/29/business/st...   \n",
       "1  https://www.channelnewsasia.com/singapore/su-w...   \n",
       "2  https://edition.cnn.com/2023/05/22/tech/meta-f...   \n",
       "3  https://www.channelnewsasia.com/singapore/bill...   \n",
       "4  https://edition.cnn.com/2024/03/05/politics/li...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Starbucks violated federal labor law when it i...   \n",
       "1  The first suspect to plead guilty in Singapore...   \n",
       "2  Meta has been fined a record-breaking €1.2 bil...   \n",
       "3  SINGAPORE: A 45-year-old man linked to Singapo...   \n",
       "4  The Department of Education imposed a record $...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [starbucks, violate, federal, labor, law, incr...   \n",
       "1  [suspect, plead, guilty, singapores, large, mo...   \n",
       "2  [meta, fine, recordbreaking, billion, billion,...   \n",
       "3  [singapore, yearold, man, link, singapores, la...   \n",
       "4  [department, education, impose, record, millio...   \n",
       "\n",
       "                                       relationships  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2  [{'source': 'chapter', 'relation': 'set', 'tar...  \n",
       "3                                                 []  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1428eccd",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6149d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'Text' is empty or NaN\n",
    "data = data.dropna(subset=['Text'])\n",
    "\n",
    "# Optional: Remove duplicate rows\n",
    "data = data.drop_duplicates(subset=['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980919d",
   "metadata": {},
   "source": [
    "# Testing section (getting non-grammatical r/s) -START-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57e2e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_tokenization(text):\n",
    "    \"\"\"Preprocess and tokenize text, removing unnecessary words.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.lower().strip())\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc \n",
    "        if not token.is_stop and not token.is_punct and token.is_alpha and len(token.lemma_) > 2\n",
    "    ]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def extract_advanced_entities(text):\n",
    "    \"\"\"Extract entities and keep both original & lemmatized versions for better matching.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    entities = {\n",
    "        ent.text.strip(): ent.label_  \n",
    "        for ent in doc.ents \n",
    "        if ent.label_ in ['ORG', 'GPE', 'PERSON', 'LAW', 'NORP', 'FAC', 'MONEY', 'EVENT']\n",
    "    }\n",
    "\n",
    "    entity_tokens = set(entities.keys()) | {lemma for e in entities.keys() for lemma in advanced_tokenization(e)}\n",
    "\n",
    "    return entity_tokens\n",
    "\n",
    "def extract_advanced_relationships(text):\n",
    "    \"\"\"Extract meaningful relationships while filtering out noise.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    relationships = []\n",
    "    entity_set = extract_advanced_entities(text)\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            token_lemma = token.lemma_\n",
    "\n",
    "            # **Core Subject-Verb-Object (SVO) Relationships**\n",
    "            if token.dep_ == 'nsubj' and token.head.pos_ == 'VERB':\n",
    "                obj = [child for child in token.head.children if child.dep_ in ['dobj', 'attr', 'pobj']]\n",
    "                if obj:\n",
    "                    source = token.text if token.text in entity_set else token_lemma\n",
    "                    target = obj[0].text if obj[0].text in entity_set else obj[0].lemma_\n",
    "\n",
    "                    relationships.append({\n",
    "                        'source': source,\n",
    "                        'relation': token.head.lemma_,\n",
    "                        'target': target,\n",
    "                        'type': 'SVO'\n",
    "                    })\n",
    "\n",
    "            # **Attribution (Authority → Action)**\n",
    "            if token.dep_ == 'ccomp' and token.head.ent_type_ in ['ORG', 'PERSON', 'LAW']:\n",
    "                if token.head.text in entity_set:\n",
    "                    relationships.append({\n",
    "                        'source': token.head.text,\n",
    "                        'relation': 'ruled',\n",
    "                        'target': token_lemma,\n",
    "                        'type': 'Attribution'\n",
    "                    })\n",
    "\n",
    "            # **Causal Relationships (Why something happened)**\n",
    "            if token.dep_ in ['advcl', 'ccomp'] and token.head.pos_ == 'VERB':\n",
    "                if token.head.text in entity_set or token_lemma in entity_set:\n",
    "                    relationships.append({\n",
    "                        'source': token.head.text,\n",
    "                        'relation': 'caused',\n",
    "                        'target': token_lemma,\n",
    "                        'type': 'Causal'\n",
    "                    })\n",
    "\n",
    "            # **Possession & Ownership**\n",
    "            if token.dep_ in ['poss', 'nmod', 'prep']:\n",
    "                poss_obj = [child for child in token.children if child.dep_ == 'pobj']\n",
    "                if poss_obj and token.head.text in entity_set and poss_obj[0].text in entity_set:\n",
    "                    relationships.append({\n",
    "                        'source': token.head.text,\n",
    "                        'relation': 'owns',\n",
    "                        'target': poss_obj[0].text,\n",
    "                        'type': 'Possession'\n",
    "                    })\n",
    "\n",
    "    # **Fallback Strategy: If No Relationships Found, Infer from Context**\n",
    "    if not relationships:\n",
    "        inferred_relationships = infer_relationships_from_context(doc, entity_set)\n",
    "        if inferred_relationships:\n",
    "            relationships.extend(inferred_relationships)\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_relationships = []\n",
    "    seen = set()\n",
    "    for rel in relationships:\n",
    "        key = (rel['source'], rel['relation'], rel['target'])\n",
    "        if key not in seen:\n",
    "            unique_relationships.append(rel)\n",
    "            seen.add(key)\n",
    "\n",
    "    return unique_relationships\n",
    "\n",
    "data['entities'] = data['Text'].apply(extract_advanced_entities)\n",
    "data['relationships'] = data['Text'].apply(extract_advanced_relationships)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0adf8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(data['relationships'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d21156ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>relationships</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://edition.cnn.com/2023/09/29/business/st...</td>\n",
       "      <td>Starbucks violated federal labor law when it i...</td>\n",
       "      <td>[starbucks, violate, federal, labor, law, incr...</td>\n",
       "      <td>[{'source': 'Starbucks', 'relation': 'violate'...</td>\n",
       "      <td>{labor, Mara-Louise Anzalone, NLRB, relations,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.channelnewsasia.com/singapore/su-w...</td>\n",
       "      <td>The first suspect to plead guilty in Singapore...</td>\n",
       "      <td>[suspect, plead, guilty, singapores, large, mo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{US$2.2 billion, road, Singapore, Su, cambodia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://edition.cnn.com/2023/05/22/tech/meta-f...</td>\n",
       "      <td>Meta has been fined a record-breaking €1.2 bil...</td>\n",
       "      <td>[meta, fine, recordbreaking, billion, billion,...</td>\n",
       "      <td>[{'source': 'Board', 'relation': 'announce', '...</td>\n",
       "      <td>{The European Data Protection Board, states, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.channelnewsasia.com/singapore/bill...</td>\n",
       "      <td>SINGAPORE: A 45-year-old man linked to Singapo...</td>\n",
       "      <td>[singapore, yearold, man, link, singapores, la...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{Singapore, around S$118 million, ruijin, Zhan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://edition.cnn.com/2024/03/05/politics/li...</td>\n",
       "      <td>The Department of Education imposed a record $...</td>\n",
       "      <td>[department, education, impose, record, millio...</td>\n",
       "      <td>[{'source': 'Department', 'relation': 'impose'...</td>\n",
       "      <td>{liberty, Liberty University, an additional $2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Link  \\\n",
       "0  https://edition.cnn.com/2023/09/29/business/st...   \n",
       "1  https://www.channelnewsasia.com/singapore/su-w...   \n",
       "2  https://edition.cnn.com/2023/05/22/tech/meta-f...   \n",
       "3  https://www.channelnewsasia.com/singapore/bill...   \n",
       "4  https://edition.cnn.com/2024/03/05/politics/li...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Starbucks violated federal labor law when it i...   \n",
       "1  The first suspect to plead guilty in Singapore...   \n",
       "2  Meta has been fined a record-breaking €1.2 bil...   \n",
       "3  SINGAPORE: A 45-year-old man linked to Singapo...   \n",
       "4  The Department of Education imposed a record $...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [starbucks, violate, federal, labor, law, incr...   \n",
       "1  [suspect, plead, guilty, singapores, large, mo...   \n",
       "2  [meta, fine, recordbreaking, billion, billion,...   \n",
       "3  [singapore, yearold, man, link, singapores, la...   \n",
       "4  [department, education, impose, record, millio...   \n",
       "\n",
       "                                       relationships  \\\n",
       "0  [{'source': 'Starbucks', 'relation': 'violate'...   \n",
       "1                                                 []   \n",
       "2  [{'source': 'Board', 'relation': 'announce', '...   \n",
       "3                                                 []   \n",
       "4  [{'source': 'Department', 'relation': 'impose'...   \n",
       "\n",
       "                                            entities  \n",
       "0  {labor, Mara-Louise Anzalone, NLRB, relations,...  \n",
       "1  {US$2.2 billion, road, Singapore, Su, cambodia...  \n",
       "2  {The European Data Protection Board, states, t...  \n",
       "3  {Singapore, around S$118 million, ruijin, Zhan...  \n",
       "4  {liberty, Liberty University, an additional $2...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21b05e",
   "metadata": {},
   "source": [
    "# Testing section -END-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ff451",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a37c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the 'Text' column\n",
    "data['tokens_1'] = data['Text'].apply(lambda x: [token.text for token in nlp(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489006b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['tokens_1'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4267348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_tokenization(text):\n",
    "    # Preprocessing\n",
    "    text = re.sub(r'\\s+', ' ', text.lower().strip())\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # SpaCy tokenization\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Enhanced token filtering\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc \n",
    "        if not token.is_stop and \n",
    "        not token.is_punct and \n",
    "        token.is_alpha and \n",
    "        len(token.lemma_) > 2\n",
    "    ]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "data['tokens'] = data['Text'].apply(advanced_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfaf404",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['tokens'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4589b",
   "metadata": {},
   "source": [
    "# Extract Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23d20d1",
   "metadata": {},
   "source": [
    "Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee517f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text.strip(), ent.label_) for ent in doc.ents] #.strip() to get rid of whitespaces\n",
    "    return entities\n",
    "\n",
    "# Apply to the dataset\n",
    "data['entities1'] = data['Text'].apply(extract_entities_spacy)\n",
    "\n",
    "data['filtered_entities'] = data['entities1'].apply(\n",
    "    lambda entities: [ent for ent in entities if ent[1] in ['ORG', 'GPE', 'PERSON']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['filtered_entities'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f85c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "def advanced_entity_deduplication(entities):\n",
    "    \"\"\"\n",
    "    Advanced entity deduplication with smart merging strategies\n",
    "    \"\"\"\n",
    "    def normalize_entity(entity):\n",
    "        # Comprehensive normalization\n",
    "        normalized = re.sub(r'\\s+', ' ', entity.lower().strip())\n",
    "        normalized = re.sub(r'[\\'\"]', '', normalized)  # Remove quotes\n",
    "        normalized = re.sub(r'\\bthe\\b', '', normalized)  # Remove 'the'\n",
    "        return normalized\n",
    "    \n",
    "    def choose_best_entity(candidates):\n",
    "        # Enhanced preference rules for entity selection\n",
    "        preferences = [\n",
    "            lambda x: 'global' in x or 'board' in x or 'relations' in x,  # Prefer comprehensive names\n",
    "            lambda x: len(x.split()) > 1,  # Prefer longer names\n",
    "            lambda x: not x.isdigit(),  # Prefer non-numeric\n",
    "            lambda x: x  # Fallback to first candidate\n",
    "        ]\n",
    "        \n",
    "        for pref in preferences:\n",
    "            matches = [e for e in candidates if pref(e)]\n",
    "            if matches:\n",
    "                return max(matches, key=len)\n",
    "    \n",
    "    # Extract entity names from tuples if needed\n",
    "    if entities and isinstance(entities[0], tuple):\n",
    "        entities = [ent[0] for ent in entities]\n",
    "    \n",
    "    # Advanced fuzzy matching and grouping\n",
    "    deduplicated = []\n",
    "    while entities:\n",
    "        current = entities.pop(0)\n",
    "        norm_current = normalize_entity(current)\n",
    "        \n",
    "        matches = [\n",
    "            ent for ent in entities \n",
    "            if (fuzz.ratio(normalize_entity(ent), norm_current) > 90 or \n",
    "                normalize_entity(ent) in norm_current or \n",
    "                norm_current in normalize_entity(ent))\n",
    "        ]\n",
    "        \n",
    "        group = [current] + matches\n",
    "        best_entity = choose_best_entity(group)\n",
    "        deduplicated.append(best_entity)\n",
    "        \n",
    "        # Remove matched entities\n",
    "        entities = [ent for ent in entities if ent not in matches]\n",
    "    \n",
    "    return list(set(deduplicated))\n",
    "\n",
    "# Apply to your dataframe\n",
    "data['deduplicated_entities'] = data['filtered_entities'].apply(advanced_entity_deduplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['deduplicated_entities'][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d4165",
   "metadata": {},
   "source": [
    "Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f3378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_advanced_entities(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract entities with confidence scoring\n",
    "    entities = [\n",
    "        {\n",
    "            'text': ent.text.strip(), \n",
    "            'label': ent.label_,\n",
    "            'confidence': round(ent._.confidence, 2) if hasattr(ent, '_.confidence') else 0.8\n",
    "        } \n",
    "        for ent in doc.ents \n",
    "        if ent.label_ in ['ORG', 'GPE', 'PERSON'] #See whether to include FAC, etc\n",
    "    ]\n",
    "    \n",
    "    # Remove duplicates while preserving most confident entries\n",
    "    unique_entities = []\n",
    "    seen = set()\n",
    "    for entity in sorted(entities, key=lambda x: x['confidence'], reverse=True):\n",
    "        normalized = re.sub(r'\\s+', ' ', entity['text'].lower())\n",
    "        if normalized not in seen:\n",
    "            unique_entities.append(entity)\n",
    "            seen.add(normalized)\n",
    "    \n",
    "    return unique_entities\n",
    "\n",
    "data['entities'] = data['Text'].apply(extract_advanced_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b92d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['entities'][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd35c3",
   "metadata": {},
   "source": [
    "# Extract Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3908679d",
   "metadata": {},
   "source": [
    "With spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b22f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relationships(text):\n",
    "    doc = nlp(text)\n",
    "    relationships = []\n",
    "    \n",
    "    # Extract all entities, not just limiting to specific types\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            # Broader dependency relations\n",
    "            if token.dep_ in ['nsubj', 'dobj', 'pobj', 'attr', 'agent', 'nmod', 'conj', 'ROOT']:\n",
    "                # Check if head or token is an entity\n",
    "                if (token.head.text in entities or token.text in entities):\n",
    "                    relationships.append({\n",
    "                        'source': token.head.text,\n",
    "                        'target': token.text,\n",
    "                        'relation': token.dep_,\n",
    "                        'sentence': sent.text\n",
    "                    })\n",
    "    \n",
    "    return relationships\n",
    "\n",
    "# Apply relationship extraction\n",
    "data['relationships1'] = data['Text'].apply(extract_relationships)\n",
    "\n",
    "# Print relationships to debug\n",
    "print(data['relationships1'].iloc[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccbcdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_advanced_relationships(text):\n",
    "    doc = nlp(text)\n",
    "    relationships = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            # Broader semantic relationship extraction\n",
    "            if token.dep_ in ['nsubj', 'dobj', 'attr', 'agent', 'nmod', 'conj', 'ROOT']:\n",
    "                relationships.append({\n",
    "                    'source': token.head.text,\n",
    "                    'target': token.text,\n",
    "                    'relation': token.dep_,\n",
    "                    'pos_source': token.head.pos_,\n",
    "                    'pos_target': token.pos_\n",
    "                })\n",
    "    \n",
    "    return relationships\n",
    "\n",
    "data['relationships'] = data['Text'].apply(extract_advanced_relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a6ee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['relationships'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052f5a74",
   "metadata": {},
   "source": [
    "# Validate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data_quality(data):\n",
    "    validation_report = {\n",
    "        'total_documents': len(data),\n",
    "        'documents_with_entities': sum(len(row['entities']) > 0 for row in data),\n",
    "        'documents_with_relationships': sum(len(row['relationships']) > 0 for row in data),\n",
    "        'avg_entities_per_doc': np.mean([len(row['entities']) for row in data]),\n",
    "        'avg_relationships_per_doc': np.mean([len(row['relationships']) for row in data])\n",
    "    }\n",
    "    \n",
    "    return validation_report\n",
    "\n",
    "\n",
    "validation_results = validate_data_quality(data.to_dict('records'))\n",
    "print(\"Data Validation Report:\")\n",
    "for key, value in validation_results.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9aa374",
   "metadata": {},
   "source": [
    "# Plot Entity Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dff1121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entity_network(relationships):\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add edges based on relationships\n",
    "    for rel in relationships:\n",
    "        G.add_edge(rel['source'], rel['target'], type=rel['relation'])\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "    nx.draw_networkx_nodes(G, pos, node_color='lightblue', node_size=500, alpha=0.8)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray', alpha=0.5)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "    plt.title(\"Entity Relationship Network\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6978e4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_entity_network(data['relationships'][5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a3d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entity_distribution(entities):\n",
    "    # Count entities by type\n",
    "    entity_types = [entity['label'] for entity in entities]\n",
    "    type_counts = Counter(entity_types)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(type_counts.keys(), type_counts.values())\n",
    "    plt.title('Entity Type Distribution')\n",
    "    plt.xlabel('Entity Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_entity_distribution(data['entities'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7002fd9c",
   "metadata": {},
   "source": [
    "# Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d6f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_insights(data):\n",
    "    insights = {\n",
    "        'top_entities': Counter([\n",
    "            entity['text'] for doc_entities in data['entities'] \n",
    "            for entity in doc_entities\n",
    "        ]).most_common(10),\n",
    "        'most_common_relationships': Counter([\n",
    "            rel['relation'] for doc_relationships in data['relationships'] \n",
    "            for rel in doc_relationships\n",
    "        ]).most_common(5)\n",
    "    }\n",
    "    return insights\n",
    "\n",
    "key_insights = extract_key_insights(data)\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"Top Entities:\", key_insights['top_entities'])\n",
    "print(\"Most Common Relationship Types:\", key_insights['most_common_relationships'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f8c023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
